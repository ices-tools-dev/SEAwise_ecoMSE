---
title: "Model evaluation and selection"

header-includes:
  \usepackage{caption}
  \usepackage{float}

output:
#  pdf_document:
#    number_sections: yes
#    fig_caption: yes
  html_document:
    number_sections: yes
    fig_caption: yes
    df_print: paged
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
    self_contained: true
      
bibliography: modelselection.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = T,
  echo = T,
  fig.width = 6, fig.height = 5,
  fig.pos = "H"
)
```

# Required packages

```{r required_packages, message=FALSE, warning=FALSE}
# install.packages(c("corrplot", "doRNG", "dplyr", "forecast", "GA", "ggplot2", "glmmTMB", "knitr", "memoise", "MuMIn", "parallel", "patchwork", "tidyr"))

library(corrplot)
library(doRNG)
library(dplyr)
library(forecast)
library(GA)
library(ggplot2)
library(glmmTMB)
library(knitr)
library(memoise)
library(MuMIn)
library(parallel)
library(patchwork)
library(tidyr)
```

# Introduction

Considerations when choosing a "best" model:

-   **We want a model that fits the observed data well**
-   **We want a model that makes biological sense**
    -   Includes plausible covariates, even if the precise mechanism is not known
-   **We want a model that can make good predictions (in the future)**
    -   Especially important in MSE context where forecasts are run
    -   There may be special considerations when covariates extend beyond the observed range
-   Metrics
    -   Internal model metrics (AIC, LogLik)

# Synthetic data creation

We will generate the data used in the example for simplicity and to gain insight into the model selection process, as we will know the "true" underlying relationship. The example shown here will be to fit an environmentally-mediated stock-recruitment relationship (EMSRR), using a modified Ricker ["controlling"-type as described by @levi_embedding_2003]

## Environmental covariates

First we define some environmental covariates using an autoregressive integrated moving average (ARIMA) model. Here we will model only AR1 processes, which is the degree of correlation of subsequent values based on prior values.

```{r genCovs, fig.height = 6}
set.seed(1235)
n <- 70
ncov <- 12
AR <- runif(ncov, min = 0, max = 0.9) # auto-regression coefficients

envvars <- paste0("x", formatC(x = seq(ncov), digits = 1, flag = 0))
tmp <- lapply(seq(ncov), FUN = function(i){
  # Generate the time series
  ts_data <- arima.sim(model = list(ar = AR[i]), n = n)
  return(ts_data)
})

tmp <- as.data.frame(matrix(unlist(tmp), nrow = n, ncol = ncov))
names(tmp) <- envvars
dat <- cbind(data.frame(t = seq(n)), tmp)

# add a trend to x01 and x02
dat$x01 <- dat$x01 + diff(range(dat$x01))*0.01*(dat$t-mean(dat$t)) # 1% of sd per year
dat$x02 <- dat$x02 + diff(range(dat$x02))*0.01*(dat$t-mean(dat$t)) # 1% of sd per year

# plot time series
df <- tidyr::pivot_longer(dat, cols = seq(ncol(dat))[-1])
ggplot(df) + aes(x = t, y = value, fill = name, color = name) + 
  geom_area(show.legend = F) + 
  facet_wrap(~ name, ncol = 2, scales = "free_y") + 
  theme_bw()
```

## Biological covariates and response

Generate ssb and rec

```{r genBioVars, fig.height=3}
# generate ssb (random walk)
set.seed(236)
dat$ssb <- 30000 - cumsum(rnorm(n, 0, 3000))

# generate recruitment (Ricker model + env mediation from x01 & x03)
dat$rec <- exp(log(5) + log(dat$ssb) - 2.5e-5*dat$ssb + 0.05*dat$x01 + 0.05*dat$x03)
dat$rec <- dat$rec * rlnorm(n, 0, 0.2) # additional random noise

p1 <- ggplot(dat) + aes(x = t, y = ssb) + 
  geom_line()
p2 <- ggplot(dat) + aes(x = t, y = rec) + 
  geom_line()
p3 <- ggplot(dat) + aes(x = ssb, y = rec) + 
  geom_point() +
  lims(x = c(0, NA), y = c(0, NA))

layout <- "
AC
BC
"
p <- p1 + p2 + p3 + plot_layout(design = layout, axis_titles = "collect") & 
  theme_bw()
p
```

## Resulting correlations among variables

```{r corrPlot}
corrplot.mixed(cor(dat[,-1]))
```

## Full model definition

Ricker with 
[@levi_embedding_2003; @iles_stock_1998; @akimova_spatially-resolved_2016; @iles_review_1994]

Concept of penalization (degrees of freedom)

ANOVA comparison (`summary`) - single term drops

```{r fullModel}
fmla <- formula(paste(c("rec ~ offset(log(ssb)) + ssb", envvars), collapse = " + "))
fmla
fit0 <- glm(fmla, data = dat, family = gaussian(link = "log"))
summary(fit0)
```
# Model selection with Akaike information criterion
A large part of model building is selecting the best model to describe the response variable. An important concept to remember when looking for the "best" model is whether the addition of terms improves the predictive power of the model. Generally, the more explanatory variables that one has, the more variation will be explained in the response variable, but one has to be careful not to include variables that do not explain a *significant* portion of the variation. The inclusion of these terms might help you achieve a higher $R^2$ value for your specific data set, but are likely to be a poor predictor of new data that was not seen during the fitting. This problem is referred to as "*overfitting*".

The strategy presented here relies on the comparison of "*nested models*" in determining whether a term should be kept or not. Nested models differ by a single term, whereby the larger model contains all of the terms of the smaller model, plus an additional one. One commonly used strategy, is to start with your most complex model (e.g. the model above that included species, petal length, and their interaction) and work backwards, trying to sequentially remove variables that are not significant. 

As a general rule, one should start by removing higher-order terms first (e.g. interaction terms) before moving on main terms or lower-level nested terms. Furthermore, some believe that there is a general rule stating that the main term should always be included if the term is also used in an interaction (see discussion [here](http://stats.stackexchange.com/q/11009/10675)).

This logic is within the automated stepwise search routine found in the function `step()` (introduced below), which usually makes sense, however it is definitely not a hard and fast rule. A basic guideline might be to try and formulate models are plausible given your specific question, and treat the inclusion of terms as hypotheses to be tested (see discussion [here](http://stats.stackexchange.com/q/4901/10675)). 

## Stepwise search

Let's compare fit0 and fit with AIC and cross validation metrics

By default the `step` function uses a `direction = "backward"` search, which starts with the largest model and removes terms gradually according to the largest improvements in AIC. When `direction = "both"`, terms may come back into the model at later steps. With models of  strategy for term re

```{r stepwiseRemoval}
# model with all original variables in fit0
all_variables <- "rec ~ ."
# Variables that should always be in the model (Ricker without env.)
fixed_variables <- "rec ~ offset(log(ssb)) + ssb"  

fit <- step(fit0, 
  scope = list(
    lower = as.formula(fixed_variables), 
    upper = as.formula(all_variables)
  ), 
  direction = "both",
  trace = FALSE # set to TRUE if you would like to see the model selections steps
)

summary(fit)
c(fit0=AIC(fit0), fit=AIC(fit)) # comparison of AIC
```

Akaike rule-of-thumb for improved model: $\delta AIC \geq 2$



## Dredging

If your model fitting is fast, and there are not too many possible combinations ($combinations = 2^n-1$, where n is the number of possible covariaes), then one could try all possible models with the `dredge` function of the package `MuMIn` (*Multi-Model Inference*), which can handle many model types. In our example, it's relatively quick to run through all model types. The function `pdredge` can also parallelize the process. 

```{r dredge}
fmla <- formula(paste(c("rec ~ offset(log(ssb)) + ssb", envvars), collapse = " + "))
# argument na.action = na.fail required for fair comparisons
fittmp <- glm(fmla, data = dat, family = gaussian(link = "log"), na.action = na.fail)

t1 <- Sys.time()
dd <- dredge(fittmp, fixed = c("offset(log(ssb))", "ssb"), trace = FALSE)
t2 <- Sys.time()
# t2 - t1

# subset(dd[1:100,], delta < 4)
plot(dd[1:100,], labAsExpr = TRUE)
```




# Visualizing marginal effects

One way to visualize the envitonmental  covariate effects, is to hold all others at their median values, and generate a range of values only for a single focus covariate. 

```{r plotRicker}
envvarsKeep <- intersect(names(coef(fit)), envvars) # maintained env. covariates
# envvarsKeep <- c("x01", "x03")
focusVar <- envvarsKeep[1] # choose covariate of focus

## generate data for prediction
# focus covariate generated at 5 quantile levels, other env covariates use median 
L <- lapply(envvarsKeep, FUN = function(x){
  if(x == focusVar){probs = c(0.05,0.25,0.5,0.75,0.95)}else{probs <- 0.5}
  quantile(dat[,x], probs = probs)
})
names(L) <- envvarsKeep
L[["ssb"]] <-  seq(0, max(dat$ssb), len = 100)

nd <- expand.grid(L)
nd$rec <- predict(fit, newdata = nd, type = "response")

ggplot(dat) + aes(x = ssb, y = rec, col = get(focusVar)) + 
  geom_point() + 
  scale_color_continuous(name = focusVar, type = "viridis") +
  geom_line(data = nd, mapping = aes(group = get(focusVar))) + 
  theme_bw()
```

# Model selection via cross validation

- Cross validation is a very flexible framework that tests how well a model can predict independent (unseen) data
- Data is typically split into two set; one for model fitting (*training data*) and one for prediction (*validation data*)
- Can be used as a basis for selecting best performing model (e.g. inclusion of terms, model structure) as it can evaluate overfitting
- Performance can be evaluated based on the desired metric


## Common metrics

Below are some common metrics that could be used for model selection. The choice of metric will depend on the goal of the model prediction. For example,

Note that these metrics may differ from those by the model's fitting criteria to the training data set. Consistency between the two may be desirable, although one might want to emphasize other aspects of the prediction (validation) data set.

For example, squared error metrics, like Mean Squared Error ($MSE$) and Root Mean Squared Error ($RMSE$), will more heavily penalize outliers and large errors. Mean Absolute Error $MAE$ an intuitive metric that gives average error in the same units as the data. Mean Absolute Percentage Error ($MAPE$) and Mean Absolute Scaled Error ($MASE$) provide a scale-independent metric, useful when data varies significantly in magnitude.

Below are some R functions that can return these metrics, with arguments `y` and `yhat` providing the observed and predicted response values, respectively, of the validation data.

```{r metricFuns}
# Mean Squared Error (MSE)
mse <- function(y, yhat){mean((y - yhat)^2, na.rm = TRUE)}

# Root Mean Squared Error (RMSE)
rmse <- function(y, yhat){sqrt(mean((y - yhat)^2, na.rm = TRUE))}

# Mean Absolute Percentage Error (MAPE)
mape <- function(y, yhat){mean(abs(y - yhat) / y, na.rm = TRUE)}

# Median Absolute Percentage Error (MAPE)
mdape <- function(y, yhat){median(abs(y - yhat) / y, na.rm = TRUE)}

# Mean Absolute Error (MAE)
mae <- function(y, yhat){mean(abs(y - yhat), na.rm = TRUE)}

# Mean Absolute Scaled Error (MASE)
mase <- function(y, yhat, yhat_naive){mean(abs(y - yhat), na.rm = TRUE) /
    mean(abs(y - yhat_naive), na.rm = TRUE)}
```

## Flavor 1: Repeated k-fold

Many flavors of cross validation exist, but we will focus on one of the most common and flexible ones -- *k-fold cross validation*.

In this approach, the *k* signifies the number of random groupings (i.e. *folds*) to use for the training and validation data. The groups are balanced, such that each data sample will be used once for validation.

For example, the commonly used 5-fold cross validation divides the data into 5 groups, with each fold using 4 out of 5 of the groups for training (\~80% of data) and the remaining group for validation (\~20% of data).

To increase the robustness of the prediction error estimates, the procedure can be repeated a number of times, with fold assignment changing randomly for each permutation.

### Required functions

The following functions will be used in the examples:

- `kfold()` - Randomly partitions the data into the prescribed number of folds (`k`)
- `cvFun()` - Re-fits a defined model to each of the folds defined by `kfold` and records the observed and predicted response values (inner loop), which can be repeated for a defined number of permutations (`nperm`). The setting of a random `seed` value ensures reproducibility among fold assignments, which is desired when comparing several models. 

```{r cvFuncions}
# Determines k-fold partitions for a given number of samples
# n is the number of samples; k is the number of partitions
kfold <- function(n, k = NULL){
  if(is.null(k)){k <- n} # if undefined, assume leave-one-out (LOO) CV
  fold <- vector(mode="list", k)
  n.remain <- seq(n)
  for(i in seq(k)){
    samp <- sample(seq(length(n.remain)), ceiling(length(n.remain)/(k-i+1)))
    fold[[i]] <- n.remain[samp]
    n.remain <- n.remain[-samp]
  }
  return(fold)
}

# perform repeated k-fold cross validation
# n and k are passed to kfold; seed is a integer value passed to set.seed
cvFun <- function(model = NULL, nperm = 10, k = 5, seed = 1){
  modelData <- model.frame(model)
  y <- model.response(model.frame(model))
  fmla <- formula(model)
  # when k = number of sample, a single leave-one-out cross validation is performed (LOOCV)
  if(k == length(y)){nperm <- 1} 
  res <- vector("list", nperm)
  set.seed(seed)
  for(j in seq(nperm)){ # for each permutation j
    fold <- kfold(n = nrow(modelData), k = k)
    res[[j]] <- data.frame(perm = j, fold = NaN, y = y, yhat = NaN)
    for(i in seq(fold)){ # for each fold
      train <- -fold[[i]]
      valid <- fold[[i]]
      # update model with training data
      fit.fold <- update(object = model, data = modelData[train,], 
        formula = as.formula(fmla, env = environment())) 
      # predict validation data
      res[[j]]$yhat[valid] <- predict(object = fit.fold, 
        newdata = modelData[valid,], type = "response")
      res[[j]]$fold[valid] <- i
    }
  }
  res <- do.call("rbind", res)
  return(res)
}
```

### Fitting to each fold

The following figure shows how the data samples in a 5-fold cross validation are randomly assigned to the folds (colors) as validation data sets. For each fold, error is assessed based on the predicted (lines) versus observed (points) from a given fold.


```{r kfoldExample, echo=FALSE}
fold <- kfold(n = nrow(dat), k = 5)
df <- as.data.frame(do.call("cbind", fold))
names(df) <- seq(ncol(df))

df <- pivot_longer(df, cols = 1:5)
df <- df[order(df$value),]
names(df) <- c("fold", "t")

df = merge(dat, df, all = TRUE)

newdat <- data.frame(ssb = seq(0, max(df$ssb), len = 100))
pred <- vector("list", length(fold))
for (i in seq_along(fold)) {
  train <- -fold[[i]]
  valid <- fold[[i]]
  fit.fold <- update(fit0, data = df[train,], formula = as.formula(fixed_variables, env = environment()))
  newdat.i <- newdat
  newdat.i$fold <- as.character(i)
  newdat.i$rec <- predict(fit.fold, newdata = newdat.i, type = "response")
  pred[[i]] <- newdat.i
}
pred <- do.call("rbind", pred)

p1 <- ggplot(df) + aes(x = t, y = 0, col = fold) + 
  geom_point() +
  scale_color_brewer(name = "Fold", palette = "Set1") +
  labs(y = "") +
  scale_y_continuous(limits = c(0, 0), breaks = NULL) + 
  theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
    axis.ticks.y = element_blank(), axis.line.y = element_blank())

p2 <- ggplot(df) + aes(x = ssb, y = rec, col = fold) + 
  geom_point() +
  geom_line(data = pred, show.legend = F) +
  lims(x = c(0,NA), y = c(0,NA)) +
  scale_color_brewer(name = "Fold", palette = "Set1")

layout = "
AA
BB
BB
"

p <- (p1 / p2) + plot_layout(design = layout, guides = "collect") &
  theme_bw()
print(p)
```


### Influence of fold number on error

The number of folds chosen depends on several considerations; such as the variance in the data, number of samples, and computational cost. A low k (e.g. 5-fold) typically provides a good balance for the types of models that we have looked at. The larger amount of validation data (20\%) provides a robust measure of prediction error, and the low number of folds is not computationally costly. That said, performing repeated k-fold cross validation (i.e. multiple permutations) will further reduce variability in error estimation.

The number of folds will influence the resulting metric of fit to the validation set. The more folds, the closer one gets to the error of the data, with a leave-one-out cross validation (*LOOCV*) being the least biased.

In the following example, a simple linear model is evaluated by repeated 5-fold cross validation. As the number of folds increases, the estimated error approaches the true error. The vertical lines show the standard error around each RMSE estimate, based on the distribution of values from the permutations.

Note that model error (based on residuals) tends to underestimate the true prediction error because it is calculated on the same data used to train the model. This can lead to overfitting, where the model appears to perform better on the training data than it would on unseen data. 

LOOCV provides a *nearly* unbiased estimate of the prediction error because each data point is excluded from the training set when its error is calculated. This does not mean that the underlying model couldn't still suffer from overfitting.


```{r kfoldErrorEst, echo=FALSE}
n <- 20
set.seed(123)
x <- runif(n, min = 0, max = 40)
err <- rnorm(n, sd = 15)
y <- 10 + 2*x + err
df <- data.frame(x, y)
fitlm <- lm(y ~ x, df)

res <- data.frame(k = c(round(exp(seq(log(2),log(n/2), len = 10))),n), rmse = NaN, lwr = NaN, upr = NaN)
for(i in seq(nrow(res))){
  tmp <- cvFun(model = fitlm, nperm = 50, k = res$k[i], seed = 1)
  res$rmse[i] <- rmse(y = tmp$y, yhat = tmp$yhat) # sd of residuals
  byPerm <- tmp %>% group_by(perm) %>% summarise(rmse = rmse(y = y, yhat = yhat))
  res[i,c("lwr", "upr")] <- quantile(byPerm$rmse, probs = c(0.05, 0.95))
}

p1 <- ggplot(df) + aes(x = x, y = y) + 
  geom_point() + 
  geom_abline(intercept = coef(fitlm)[1], slope = coef(fitlm)[2])

p2 <- ggplot(res) + aes(x = k, y = rmse) + 
  geom_line() + 
  geom_point() + 
  geom_segment(mapping = aes(yend = lwr)) +
  geom_segment(mapping = aes(yend = upr)) +
  geom_hline(yintercept = sqrt(mean(fitlm$residuals^2)), linetype = 2, col = 2) +
  geom_hline(yintercept = sqrt(mean(err^2)), linetype = 2, col = 4) + 
  geom_text(x = n/2, y = sqrt(mean(fitlm$residuals^2)), 
    label = "Model error", col = 2, vjust = -0.25) + 
  geom_text(x = n/2, y = sqrt(mean(err^2)), 
    label = "True error", col = 4, vjust = -0.25)

p <- (p1 | p2) & theme_bw()
print(p)
  
```

## Flexible model selection

You can compare models of different structure as well. For example, AIC may not be comparable between models of different structure, e.g. mixed models with random effects. Comparison should be made using same folds (e.g. use of random seed). A variety of metrics could be used for the final model selection.

```{r cvModelCompare, warning=FALSE}
res0 <- cvFun(model = fit0, nperm = 5, k = 5, seed = 1111)
res <- cvFun(model = fit, nperm = 5, k = 5, seed = 1111)

# mixed model with autoregressive term on time (t)
fmlatmp <- formula(fit)
fmlaMM <- as.formula(paste(c(deparse(fmlatmp), "+ ar1(t + 0 | 1)")))
fitMM <- glmmTMB(fmlaMM, data = dat, family = gaussian(link = "log"))
resMM <- cvFun(model = fitMM, nperm = 5, k = 5, seed = 1111)

cvStats0 <- with(res0, c(
  mse = mse(y = y, yhat = yhat),
  rmse = rmse(y = y, yhat = yhat),
  mape = mape(y = y, yhat = yhat),
  mdape = mdape(y = y, yhat = yhat),
  mae = mae(y = y, yhat = yhat)
))

cvStats <- with(res, c(
  mse = mse(y = y, yhat = yhat),
  rmse = rmse(y = y, yhat = yhat),
  mape = mape(y = y, yhat = yhat),
  mdape = mdape(y = y, yhat = yhat),
  mae = mae(y = y, yhat = yhat)
))

cvStatsMM <- with(resMM, c(
  mse = mse(y = y, yhat = yhat),
  rmse = rmse(y = y, yhat = yhat),
  mape = mape(y = y, yhat = yhat),
  mdape = mdape(y = y, yhat = yhat),
  mae = mae(y = y, yhat = yhat)
))

tmp <- cbind(cvStats0, cvStats, cvStatsMM)
tmp <- as.data.frame(t(apply(tmp, 1, FUN = function(x){
  paste0(format(x, scientific = TRUE, digits = 3), 
    ifelse(x == min(x, na.rm = T), "*", " "))
})))

names(tmp) <- c("res0", "res", "resMM")
tmp
```

In this case, the addition of the random intercept with an autoregressive structure based on time (t) did not improve the model for any of the evaluated metrics.

# Automated model selection with machine learning

Depending on the number of potential candidate models, as well as the complexity of the model fitting and cross validation procedures, model selection can become computationally costly. Thus, a brute force approach of evaluating all models with cross validation could be unrealistic. 

In the example by @kuhn_using_2021, a large number of candidate covariates (n = 82) were chosen based on hypothesized biological mechanisms affecting recruitment of North Sea cod. These included time series signals extracted from environmental fields (e.g. principal components from temperature, salinity, and currents) at different temporal periods (and lags). 

The approach used an efficient search algorithm called *non-dominated sorting genetic algorithm* (NSGA-II), combined with cross validation, to explore the large number of potential model configurations (n = `r formatC(2^82, 3)`). 

In the following example, we will explore a simplified version of the approach using a simpler implementation of a genetic algorithm.


## Fitness function

One starts with a *fitness function*, whose first argument is a binomial vector indicating which covariates to include (i.e. turn on/off) in the model. The main output should be a single value of individual 

```{r fitnessFun}
fitnessFun <- function(genes = NULL, data = NULL, nperm = 10, k = 5, 
  ffSeed = 1, fitnessOnly = TRUE){

  fmla <- as.formula(
    paste(c("rec ~ offset(log(ssb)) + ssb", envvars[as.logical(genes)]), collapse = " + "), 
    env = environment())
  fit <- glm(formula = fmla, data = data, family = gaussian(link = "log"))
  
  res <- cvFun(model = fit, nperm = nperm, k = k, seed = ffSeed)
  
  fitness <- -1 * mape(y = res$y, yhat = res$yhat) # to maximize, so reverse sign 
  if(!fitnessOnly){
    byGroup <- res %>% group_by(perm) %>% summarise(mape = -1*mape(y = y, yhat = yhat))
    qs <- quantile(byGroup$mape, probs = c(0.05, 0.95))
  }
  if(fitnessOnly){
    out <- fitness
  }else{
    out <- list(fitness = fitness, byGroup = byGroup, lwr = qs[1], upr = qs[2])
  }
  return(out)
}
```

```{r startPop}
popSize <- max(30, length(envvars)) # number of individuals

PARS <- seq(envvars)*0
startPop <- matrix(0, ncol = length(PARS), nrow = popSize)
diag(startPop[seq(PARS),]) <- 1
if(popSize > length(PARS)){
  hit <- seq(nrow(startPop))[-seq(PARS)]
  startPop[hit, ] <- t(apply(startPop[hit, ], 1, 
    FUN = function(x){x[sample(seq(PARS), size = 2)] <- 1; x}))
}

tmp <- startPop
tmp[] <- c("off", "on")[c(tmp)+1]
df <- as.data.frame(tmp)
names(df) <- envvars
df$ind <- seq(nrow(df))

df_long <- df %>%
  pivot_longer(cols = -ind, names_to = "variable", values_to = "value")
  
ggplot(df_long, aes(x = variable, y = ind, fill = factor(value))) +
  geom_tile(colour = 1) +
  scale_fill_manual(name = "Gene state", 
    values = c("black", "white"), breaks = c("on", "off")) +
  scale_y_reverse(expand = c(0, 0)) + 
  scale_x_discrete(expand = c(0, 0)) +
  labs(x = "Genes (covariates)", y = "Individuals", 
    title = "Starting population") +
  theme_bw()

```

Distribution of fitness for starting population


```{r}

tmp1 <- apply(startPop, 1, FUN = function(x){fitnessFun(x, data = dat)})
tmp2 <- apply(startPop, 1, FUN = function(x){paste(envvars[as.logical(x)], collapse = "+")})
tmp <- data.frame(ind = seq(tmp1), fitness = tmp1, covariates = tmp2)

ggplot(tmp) + aes(x = ind, y = fitness, label = covariates) + 
  geom_point() + 
  geom_text(angle = 90, hjust = -0.1, vjust = 0.5) + 
  scale_y_continuous(expand = expansion(mult = c(0.05,0.25))) + 
  theme_bw()

```

The fitness of the model with "x01" alone is already a clear standout with high fitness (-MAPE).

## Genetic algorithm

Next, we will provide this starting population to a genetic algorithm (`GA::ga()`) of type "binary", which will turn off/on the covariates, and test the resulting fitness. According to the [vignette of the GA package](https://cran.r-project.org/web/packages/GA/vignettes/GA.html):

> *Genetic algorithms (GAs) are stochastic search algorithms inspired by the basic principles of biological evolution and natural selection. GAs simulate the evolution of living organisms, where the fittest individuals dominate over the weaker ones, by mimicking the biological mechanisms of evolution, such as selection, crossover and mutation.*

Likewise, these aspects can be controlled by the `ga` function with secific arguments, e.g. the probability of gene crossover (`pcrossover`), the probability of mutation (`pmutation`), etc.

```{r message=FALSE}
# extra diagnostic data object to save all generations
postfit <- function(object, ...){
  pop <- as.data.frame(object@population)
  names(pop) <- paste0("par", seq(ncol(pop)))
  pop$fitness <- object@fitness
  pop$iter <- object@iter
  # update info
  if(!exists(".pop", envir = globalenv()))
    assign(".pop", NULL, envir = globalenv())
  .pop <- get(".pop", envir = globalenv())
  assign(".pop", rbind(.pop, pop), envir = globalenv()) 
  # output the input ga object (this is needed!)
  object 
}

mfitnessFun <- memoise(fitnessFun) # a memoised version of the fitness function
.pop <- NULL # make sure diagnostic output is empty
t1 <- Sys.time()
ga.fit <- ga(
  type = "binary", # for covariate selection
  fitness = mfitnessFun, # fitness function
  nBits = length(envvars), # number of genes (covariates)
  suggestions = startPop, # starting population suggestions
  popSize = popSize, # population size
  pcrossover = 0.8, # probability of gene crossover
  pmutation = 0.2,  # probability of mutation
  elitism = popSize*0.1, # number of best fitness individuals to survive at each generation
  parallel = FALSE, # set to TRUE for parallel computing (memoisation may be less effective)
  maxiter = 100, # max number of generations
  run = 40, # the max number of generations without any improvement
  monitor = FALSE, # set to TRUE to monitor fitness evolution
  data = dat, # input data
  seed = 1, # for reproducibility
  postFitness = postfit # extra diagnostics function
)
t2 <- Sys.time()
# t2 - t1 # elapsed time
tmp <- forget(mfitnessFun) # to stop memoisation

plot(ga.fit) # plot fitness development
```

## Pareto front

Rules of thumb (Tibshirani)

```{r paretoFront}
.pop$nVar <- rowSums(.pop[,seq(envvars)])
.pop$fracVar <- .pop$nVar / length(envvars)
names(.pop)[seq(envvars)] <- envvars

# pareto front
agg <- aggregate(fitness ~ fracVar, data = .pop, FUN = max)
parFront <- merge(x = agg, y = .pop, all.x = T)
parFront <- unique(parFront[,c(envvars, "nVar", "fracVar", "fitness")])
parFront <- parFront[order(parFront$nVar),]

# determine distribution of fitness values on pareto front (by permutation)
parFront$lwr <- parFront$upr <- NaN
for(i in seq(nrow(parFront))){
  genes.i <- parFront[i,envvars]
  tmp <- fitnessFun(genes = genes.i, data = dat, fitnessOnly = F)
  parFront$lwr[i] <- tmp$lwr
  parFront$upr[i] <- tmp$upr
}

# check if model fitness is greater than the upper bound of the simpler model
better <- parFront$fitness > c(-Inf, parFront$upr[-nrow(parFront)]) 
parFront$best <- FALSE
hit <- max(which(better))
parFront$best[hit] <- TRUE

# get formula of best model
best <- subset(parFront, best)[envvars]
best <- paste(c(fixed_variables, names(best)[best == 1]), collapse = " + ")

# return
parFront
```

```{r paretoFrontPlot}
ggplot(.pop) + aes(x = -fitness, y = fracVar, color = iter) + 
  geom_point(size = 3) + 
  geom_point(data = parFront, pch = 1, size = 3, color = 4, stroke = 1) + 
  scale_color_continuous(name = "Iteration", type = "viridis", direction = -1) +
  geom_path(data = parFront[better,], color = 4, linetype = 2) +
  geom_label(data = parFront[1,], color = 1, label = "Ricker", 
    hjust = 0.5, vjust = -0.5) +
  geom_segment(data = parFront, aes(x = -lwr, xend = -upr, y = fracVar, yend = fracVar), 
    color = 4, linewidth = 1) +
  geom_text(data = subset(parFront, best), color = "red", label = "*", size = 10, vjust = 0.25) +
  labs(x = "MAPE", y = "Fraction of environmental covariates", title = "Pareto front") + 
  theme_bw()
```


**Results:**

-   Best model is `r best`.
-   We evaluated a population of `r popSize` individuals for `r max(.pop$iter)` generations, totaling `r nrow(.pop)` fitness evaluations.
-   Memoisation evaluated `r nrow(unique(.pop[,seq(envvars)]))` unique models, saving us `r paste0(round((1-(nrow(unique(.pop[,seq(envvars)]))/nrow(.pop)))*100), "%")` of fitness evaluations.
-   Only `r nrow(unique(.pop[,seq(envvars)]))` model configurations out of a total possible `r 2^length(envvars)-1` were evaluated (`r paste0(round(nrow(unique(.pop[,seq(envvars)]))/(2^length(envvars)-1)*100), "%")`).

## Flavor 2: Temporal blocking

To address potential residual autocorrelation

There is autocorrelation evident in the response variable recruitment

```{r acfRec}
acf(dat$rec)
```

But this is largely explained by the covariates considered (e.g. fluctuations in SSB)

```{r}
fitRicker <- glm(rec ~ offset(log(ssb)) + ssb, data = dat, gaussian(link = "log"))
acf(resid(fitRicker))
```

```{r randomBlocks, echo=FALSE}
minSize <- 6

blockfold <- function(n, minSize){
  nBlock <- n %/% minSize # number of blocks
  leftover <- n %% minSize # remainder
  blockSize <- rep(minSize, nBlock) # define size of each block
  if(leftover > 0){ # add any leftovers
    blockSize <- blockSize + c(rep(1, leftover), rep(0, length(blockSize)-leftover))
  }
  blockSize <- sample(blockSize, length(blockSize)) # random shuffle
  idx <- inverse.rle(list(values = seq(blockSize), lengths = blockSize)) # fold index
  fold <- split(x = seq(n), idx) # create folds
  return(fold)
}

fold <- blockfold(n = nrow(dat), minSize = minSize)
df <- data.frame(t = dat$t, 
  fold = inverse.rle(list(values = seq(fold), lengths = sapply(fold, length))))

df = merge(dat, df, all = TRUE)

newdat <- data.frame(ssb = seq(0, max(df$ssb), len = 100))
pred <- vector("list", length(fold))
for (i in seq_along(fold)) {
  train <- -fold[[i]]
  valid <- fold[[i]]
  fit.fold <- update(fit0, data = df[train,], formula = as.formula(fixed_variables, env = environment()))
  newdat.i <- newdat
  newdat.i$fold <- as.character(i)
  newdat.i$rec <- predict(fit.fold, newdata = newdat.i, type = "response")
  pred[[i]] <- newdat.i
}
pred <- do.call("rbind", pred)

p1 <- ggplot(df) + aes(x = t, y = 0, col = factor(fold)) + 
  geom_point() +
  scale_color_brewer(name = "Fold", palette = "Paired") +
  labs(y = "") +
  scale_y_continuous(limits = c(0, 0), breaks = NULL) + 
  theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
    axis.ticks.y = element_blank(), axis.line.y = element_blank())

p2 <- ggplot(df) + aes(x = ssb, y = rec, col = factor(fold)) + 
  geom_point() +
  geom_line(data = pred, show.legend = F) +
  lims(x = c(0,NA), y = c(0,NA)) +
  scale_color_brewer(name = "Fold", palette = "Paired")

layout = "
AA
BB
BB
"

p <- (p1 / p2) + plot_layout(design = layout, guides = "collect") &
  theme_bw()
print(p)


```


```{r cvStatsBlock, eval=FALSE, include=FALSE}

cvFunBlock <- function(model = NULL, minSize = 5, seed = 1){
  modelData <- model.frame(model)
  y <- model.response(model.frame(model))
  fmla <- formula(model)
  set.seed(seed)
  fold <- blockfold(n = nrow(modelData), minSize = minSize)
  res <- data.frame(fold = NaN, y = y, yhat = NaN)
  for(i in seq(fold)){ # for each fold
    train <- -fold[[i]]
    valid <- fold[[i]]
    # update model with training data
    fit.fold <- update(object = model, data = modelData[train,], 
      formula = as.formula(fmla, env = environment())) 
    # predict validation data
    res$yhat[valid] <- predict(object = fit.fold, 
      newdata = modelData[valid,], type = "response")
    res$fold[valid] <- i
  }
  return(res)
}

res0 <- cvFunBlock(model = fit0, minSize = 6, seed = 1)
res <- cvFunBlock(model = fit, minSize = 6, seed = 1)


cvStats0 <- with(res0, c(
  mse = mse(y = y, yhat = yhat),
  rmse = rmse(y = y, yhat = yhat),
  mape = mape(y = y, yhat = yhat),
  mdape = mdape(y = y, yhat = yhat),
  mae = mae(y = y, yhat = yhat)
))

cvStats <- with(res, c(
  mse = mse(y = y, yhat = yhat),
  rmse = rmse(y = y, yhat = yhat),
  mape = mape(y = y, yhat = yhat),
  mdape = mdape(y = y, yhat = yhat),
  mae = mae(y = y, yhat = yhat)
))

tmp <- cbind(cvStats0, cvStats)
tmp <- as.data.frame(t(apply(tmp, 1, FUN = function(x){
  paste0(format(x, scientific = TRUE, digits = 3), 
    ifelse(x == min(x, na.rm = T), "*", " "))
})))

names(tmp) <- c("res0", "res")
tmp

```





## Flavor 3: Hindcast preformance

When future prediction is the focus, you may want to select the validation data more carefully to prevent the model from "*seeing*" the future, which could potentially bias results. One strategy is to again fit the model to the *training* data set, but have two holdout sets of future data: 1. the *validation* set, used for model selection, and 2. the *testing* set for evaluating prediction performance.


```{r testHoldout, echo=FALSE, fig.height=2}
n <- nrow(dat) # length of time series
nTest <- 5 # number of test holdout values
nValid <- 15 # number of test holdout values
nTrain <- n - nValid - nTest # number of training values
df <- dat
df$split <- c(rep("Training", nTrain), rep("Validation", nValid), rep("Testing", nTest))
df$split <- factor(df$split, levels = c("Training", "Validation", "Testing"))
ggplot(df) + aes(x = t, y = 0, col = split) + 
  geom_point() +
  scale_color_brewer(name = "Split", palette = "Set2") +
  labs(y = "") +
  scale_y_continuous(limits = c(0, 0), breaks = NULL) + 
  theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
    axis.ticks.y = element_blank(), axis.line.y = element_blank()) +
  theme_bw()
```

This can also be done in a rolling prediction manner, where the length of the training, validation, and testing sets are defined, and the starting data point can vary through time, within the limits allowed by these set lengths.

```{r rollingTestHoldout, echo=FALSE, fig.height=2}
n <- nrow(dat) # length of time series
nTest <- 5 # number of test holdout values
nValid <- 10 # number of test holdout values
nTrain <- 45 # number of training values
res <- vector("list", n)
for(i in seq(n)){
  startIdx <- i
  if((startIdx + nTrain + nValid + nTest - 1) < n){
    trainIdx <- seq(from = startIdx, len = nTrain)
    validIdx <- seq(from = startIdx + nTrain, len = nValid)
    testIdx <- seq(from = startIdx + nTrain + nValid, len = nTest)
    
    res[[i]] <- data.frame(
      split = c(rep("Training", nTrain), rep("Validation", nValid), rep("Testing", nTest)),
      t = c(trainIdx, validIdx, testIdx), 
      fold = i)
  }
}
df <- do.call("rbind", res)
df$split <- factor(df$split, levels = c("Training", "Validation", "Testing"))

ggplot(df) + aes(x = t, y = fold, col = split) + 
  geom_point() +
  scale_color_brewer(name = "Split", palette = "Set2") +
  scale_y_reverse() + 
  theme_bw()
```

```{r peelHoldout, echo=FALSE, fig.height=2}
n <- nrow(dat) # length of time series
nPeel <- 5
nTest <- 3 # number of test holdout values
nValid <- 10 # number of test holdout values
res <- vector("list", n)
for(i in seq(nPeel)){
  endIdx <- n - i +1
  testIdx <- seq(from = endIdx - nTest + 1, to = endIdx)
  validIdx <- seq(from = endIdx - nTest - nValid + 1, to = endIdx - nTest)
  trainIdx <- seq(from = 1, to = endIdx - nTest - nValid)

  res[[i]] <- data.frame(
    split = c(rep("Training", length(trainIdx)), rep("Validation", nValid), rep("Testing", nTest)),
    t = c(trainIdx, validIdx, testIdx), 
    peel = i)
}
df <- do.call("rbind", res)
df$split <- factor(df$split, levels = c("Training", "Validation", "Testing"))

ggplot(df) + aes(x = t, y = peel, col = split) + 
  geom_point() +
  scale_color_brewer(name = "Split", palette = "Set2") +
  scale_y_reverse() + 
  theme_bw()
```


### Example with synthetic data

#### Model selection


Finally, it may be useful to compare the prediction performance to some *naive* prediction, such as a mean of recent values. The *MASE* metric is of particular use for this kind of forecast comparison.

In our example, since no strong autocorrelation was detected, lets try a k-fold approach for the model selection, leaving out the final 5 years of the data for MASE evaluation versus a naive model.


```{r kfoldHindcast, echo=FALSE, fig.height=2}
n <- nrow(dat) # length of time series
k = 5
nTest <- 5 # number of test holdout values
nTrainValid <- n - nTest
res <- vector("list", 5)
ks <- kfold(n = n-nTest, k = 5)
tmp <- vector("list", k)
for(i in seq(ks)){
  df1 <- dat[seq(nTrainValid), c("t","rec")]
  df2 <- tail(dat[, c("t","rec")], nTest)
  df1$split <- NA
  df1$split[-ks[[i]]] <- "Training" 
  df1$split[ks[[i]]] <- "Validation"
  df2$split <- "Testing"
  df <- rbind(df1, df2)
  df$fold <- i
  tmp[[i]] <- df
}
df <- do.call("rbind", tmp)

df$split <- factor(df$split, levels = c("Training", "Validation", "Testing"))
# df$fold <- factor(df$fold)
ggplot(df) + aes(x = t, y = fold, color = split) + 
  geom_point() +
  scale_color_brewer(name = "Split", palette = "Set2") +
  scale_y_reverse() + 
  theme_bw()
```


```{r hindcast}
# split data into training/validation & testing
nTest <- 5
datTrainValid <- subset(dat, t <= max(t)-nTest)
datTest <- subset(dat, t > max(t)-nTest)

mfitnessFun <- memoise(fitnessFun) # a memoised version of the fitness function
.pop <- NULL # make sure diagnostic output is empty
t1 <- Sys.time()
ga.fit <- ga(
  type = "binary", # for covariate selection
  fitness = mfitnessFun, # fitness function
  nBits = length(envvars), # number of genes (covariates)
  suggestions = startPop, # starting population suggestions
  popSize = popSize, # population size
  pcrossover = 0.8, # probability of gene crossover
  pmutation = 0.2,  # probability of mutation
  elitism = popSize*0.1, # number of best fitness individuals to survive at each generation
  parallel = FALSE, # set to TRUE for parallel computing (memoisation may be less effective)
  maxiter = 100, # max number of generations
  run = 40, # the max number of generations without any improvement
  monitor = FALSE, # set to TRUE to monitor fitness evolution
  data = datTrainValid, # input data *** ONLY TRAIN/VALID ***
  seed = 1, # for reproducibility
  postFitness = postfit # extra diagnostics function
)
t2 <- Sys.time()
# t2 - t1 # elapsed time
tmp <- forget(mfitnessFun) # to stop memoisation

.pop$nVar <- rowSums(.pop[,seq(envvars)])
.pop$fracVar <- .pop$nVar / length(envvars)
names(.pop)[seq(envvars)] <- envvars

# pareto front
agg <- aggregate(fitness ~ fracVar, data = .pop, FUN = max)
parFront <- merge(x = agg, y = .pop, all.x = T)
parFront <- unique(parFront[,c(envvars, "nVar", "fracVar", "fitness")])
parFront <- parFront[order(parFront$nVar),]

# determine distribution of fitness values on pareto front (by permutation)
parFront$lwr <- parFront$upr <- NaN
for(i in seq(nrow(parFront))){
  genes.i <- parFront[i,envvars]
  tmp <- fitnessFun(genes = genes.i, data = datTrainValid, fitnessOnly = F) # Only re-fit to training/validation
  parFront$lwr[i] <- tmp$lwr
  parFront$upr[i] <- tmp$upr
}

# check if model fitness is greater than the upper bound of the simpler model
better <- parFront$fitness > c(-Inf, parFront$upr[-nrow(parFront)]) 
parFront$best <- FALSE
hit <- max(which(better))
parFront$best[hit] <- TRUE

# get formula of best model
best <- subset(parFront, best)[envvars]
best <- paste(c(fixed_variables, names(best)[best == 1]), collapse = " + ")

ggplot(.pop) + aes(x = -fitness, y = fracVar, color = iter) + 
  geom_point(size = 3) + 
  geom_point(data = parFront, pch = 1, size = 3, color = 4, stroke = 1) + 
  scale_color_continuous(name = "Iteration", type = "viridis", direction = -1) +
  geom_path(data = parFront[better,], color = 4, linetype = 2) +
  geom_label(data = parFront[1,], color = 1, label = "Ricker", 
    hjust = 0.5, vjust = -0.5) +
  geom_segment(data = parFront, aes(x = -lwr, xend = -upr, y = fracVar, yend = fracVar), 
    color = 4, linewidth = 1) +
  geom_text(data = subset(parFront, best), color = "red", label = "*", size = 10, vjust = 0.25) +
  labs(x = "MAPE", y = "Fraction of environmental covariates", title = "Pareto front") + 
  theme_bw()

```


The best model is `r best`, which we will then fit again to the full training/validation data, and predict the testing holdout. The naive model prediction will be the geometric mean of recruitment from the last 10 years.

#### Compare prediction of test set to naive model

Selected model is refit to the entire training/validation data set, and prediction is made to the test set. Here the prediction performance is presented relative to that of a naive model (MASE) to evaluate potential gains.

```{r naiveCompare}
# refit best model
fmla <- best
fitBest <- update(fit, data = datTrainValid, 
  formula = as.formula(fmla, env = environment()))
yhat <- predict(fitBest, newdata = datTest, type = "response")

# compare to geometric mean
yhat_naive_gm <- rep(exp(mean(log(tail(datTrainValid$rec, 10)))), nrow(datTest))
MASE_gm <- mase(y = datTest$rec, yhat = yhat, yhat_naive = yhat_naive_gm)

# compare to non-env Ricker
fmla <- "rec ~ ssb + offset(log(ssb))"
fitRicker <- update(fit, data = datTrainValid, 
  formula = as.formula(fmla, env = environment()))
yhat_naive_ri <- predict(fitRicker, newdata = datTest, type = "response")
MASE_ri <- mase(y = datTest$rec, yhat = yhat, yhat_naive = yhat_naive_ri)

df1 <- dat[,c("t", "rec")]
df1$type = ifelse(df1$t %in% datTrainValid$t, "train/valid", "test")
df2 <- datTest[,c("t", "rec")]
df2$rec <- yhat
df2$type = "model"
df3 <- datTest[,c("t", "rec")]
df3$rec <- yhat_naive_gm
df3$type = "Geom. mean (naive)"
df4 <- datTest[,c("t", "rec")]
df4$rec <- yhat_naive_ri
df4$type = "Ricker (naive)"

df <- rbind(df1, df2, df3, df4)
df$type <- factor(df$type, levels = c("train/valid", "test", "model", "Ricker (naive)", "Geom. mean (naive)"))

ggplot(df) + aes(x = t, y = rec, colour = type) + 
  geom_point() + 
  geom_line() +
  annotate("text", x = min(df$t), y = max(df$rec), 
    label = paste("MASE (Ricker) =", round(MASE_ri, 3)), 
    hjust = 0, color = "orange3") + 
  annotate("text", x = min(df$t), y = max(df$rec), 
    label = paste("MASE (Geom. mean) =", round(MASE_gm, 3)), 
    hjust = 0, vjust = 2, color = "green4") + 
  scale_color_manual(name = "Data type", 
    values = c("grey30", "grey", "blue", "orange3", "green4")) +
  theme_bw()

```

So, the prediction performance is slightly better than the naive prediction (MASE < 1.0), but this might not be strong enough to use in practice. Note that the model was given future SSB and the environmental covariates

Short- vs long-term performance (MSE scenario vs STF)


# Further considerations

-   Predicting outside the observed range
    -   Mechanistic-based models may be more constrained when new covariate values beyond the historical range are used for prediction
-   Detrending data?
-   NSGA-II



# Software Versions

-   `r version$version.string`
-   corrplot: `r packageVersion('corrplot')`
-   doRNG: `r packageVersion('doRNG')`
-   dplyr: `r packageVersion('dplyr')`
-   forecast: `r packageVersion('forecast')`
-   GA: `r packageVersion('GA')`
-   ggplot2: `r packageVersion('ggplot2')`
-   glmmTMB: `r packageVersion('glmmTMB')`
-   knitr: `r packageVersion('knitr')`
-   memoise: `r packageVersion('memoise')`
-   MuMIn: `r packageVersion('MuMIn')`
-   parallel: `r packageVersion('parallel')`
-   patchwork: `r packageVersion('patchwork')`
-   tidyr: `r packageVersion('tidyr')`
-   **Compiled**: `r format(Sys.Date(), '%Y-%b-%d')`


# Author

**Marc Taylor**. Thünen Institute of Sea Fisheries, Marine Living Resources Unit, Herwigstraße 31, 27572 Bremerhaven, Germany. <https://www.thuenen.de/en/sf/>

# References
