---
title: "Model evaluation and selection"

header-includes:
  \usepackage{caption}
  \usepackage{float}

output:
#  pdf_document:
#    number_sections: yes
#    fig_caption: yes
  html_document:
    number_sections: yes
    fig_caption: yes
    df_print: paged
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
    self_contained: true
      
bibliography: modelselection.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = T,
  echo = T,
  fig.width = 6, fig.height = 5,
  fig.pos = "H"
)
```

# Required packages

```{r required_packages, message=FALSE, warning=FALSE}
library(parallel)
library(forecast)
library(tidyr)
library(dplyr)
library(ggplot2)
library(corrplot)
library(GA)
library(patchwork)
library(doRNG)
library(knitr)
library(glmmTMB)
library(memoise)
```




# Introduction

Considerations when choosing a "best" model:

- **We want a model that fits the observed data well**
- **We want a model that makes biological sense**
  - Includes plausible covariates, even if the precise mechanism is not known
- **We want a model that can make good predictions (in the future)**
  - Especially important in MSE context where forecasts are run
  - There may be special considerations when covariates extend beyond the observed range



- Metrics
  - Internal model metrics (AIC, LogLik)


# Synthetic data creation

We will generate the data used in the example for simplicity and to gain insight into the model selection process, as we will know the "true" underlying relationship. The example shown here will be to fit an environmentally-mediated stock-recruitment relationship (EMSRR), using a modified Ricker ["controlling"-type as described by @levi_embedding_2003]

First we define some environmental covariates using an autoregressive integrated moving average (ARIMA) model. Here we will model only AR1 processes, which is the degree of correlation of subsequent values based on prior values.

```{r genCovs}
set.seed(1235)
n <- 70
ncov <- 12
AR <- runif(ncov, min = 0, max = 0.9) # auto-regression coefficients

envvars <- paste0("x", formatC(x = seq(ncov), digits = 1, flag = 0))
tmp <- lapply(seq(ncov), FUN = function(i){
  # Generate the time series
  ts_data <- arima.sim(model = list(ar = AR[i]), n = n)
  return(ts_data)
})

tmp <- as.data.frame(matrix(unlist(tmp), nrow = n, ncol = ncov))
names(tmp) <- envvars
dat <- cbind(data.frame(t = seq(n)), tmp)

# add a trend to x01 and x02
dat$x01 <- dat$x01 + diff(range(dat$x01))*0.01*(dat$t-mean(dat$t)) # 1% of sd per year
dat$x02 <- dat$x02 + diff(range(dat$x02))*0.01*(dat$t-mean(dat$t)) # 1% of sd per year

# plot time series
df <- tidyr::pivot_longer(dat, cols = seq(ncol(dat))[-1])
ggplot(df) + aes(x = t, y = value, fill = name, color = name) + 
  geom_area(show.legend = F) + 
  facet_wrap(~ name, ncol = 2) + 
  theme_bw()


```


Generate ssb and rec

```{r genBio, fig.height=3}
# generate ssb (random walk)
set.seed(236)
dat$ssb <- 30000 - cumsum(rnorm(n, 0, 3000))

# generate recruitment (Ricker model + env mediation from x01 & x03)
dat$rec <- exp(log(5) + log(dat$ssb) - 2.5e-5*dat$ssb + 0.05*dat$x01 + 0.05*dat$x03)
dat$rec <- dat$rec * rlnorm(n, 0, 0.2) # additional random noise

p1 <- ggplot(dat) + aes(x = t, y = ssb) + 
  geom_line()
p2 <- ggplot(dat) + aes(x = t, y = rec) + 
  geom_line()
p3 <- ggplot(dat) + aes(x = ssb, y = rec) + 
  geom_point() +
  lims(x = c(0, NA), y = c(0, NA))

layout <- "
AC
BC
"

p <- p1 + p2 + p3 + plot_layout(design = layout, axis_titles = "collect") & 
  theme_bw()
p
```

Correlations

```{r corrPlot}
corrplot.mixed(cor(dat[,-1]))
```


# Model definition  

[@levi_embedding_2003; @iles_stock_1998; @akimova_spatially-resolved_2016; @iles_review_1994]

# Standard metrics to select models

Concept of penalization (degrees of freedom)

ANOVA comparison (`summary`) - single term drops

```{r fullModel}
fmla <- formula(paste(c("rec ~ offset(log(ssb)) + ssb", envvars), collapse = " + "))
fmla
fit0 <- glm(fmla, data = dat, family = gaussian(link = "log"))
summary(fit0)
```

## Stepwise search with AIC

Let's compare fit0 and fit with AIC and cross validation metrics


```{r stepwiseRemoval}
# model with all original variables in fit0
all_variables <- "rec ~ ."
# Variables that should always be in the model (Ricker without env.)
fixed_variables <- "rec ~ offset(log(ssb)) + ssb"  

fit <- step(fit0, 
  scope = list(
    lower = as.formula(fixed_variables), 
    upper = as.formula(all_variables)
  ), 
  direction = "both",
  trace = FALSE # set to TRUE if you would like to see the model selections steps
)

summary(fit)
c(fit0=AIC(fit0), fit=AIC(fit)) # comparison of AIC

# compare the AIC of 'true' model
# AIC(update(fit, formula("~ . + x03")))
```

Akaike rule-of-thumb for improved model: $\delta AIC \geq 2$


## Visualizing marginal effects


```{r plotRicker}
envvarsKeep <- intersect(names(coef(fit)), envvars) # maintained env. covariates
# envvarsKeep <- c("x01", "x03")
focusVar <- envvarsKeep[1] # choose covariate of focus

## generate data for prediction
# focus covariate generated at 5 quantile levels, other env covariates use median 
L <- lapply(envvarsKeep, FUN = function(x){
  if(x == focusVar){probs = c(0.05,0.25,0.5,0.75,0.95)}else{probs <- 0.5}
  quantile(dat[,x], probs = probs)
})
names(L) <- envvarsKeep
L[["ssb"]] <-  seq(0, max(dat$ssb), len = 100)

nd <- expand.grid(L)
nd$rec <- predict(fit, newdata = nd, type = "response")

ggplot(dat) + aes(x = ssb, y = rec, col = get(focusVar)) + 
  geom_point() + 
  scale_color_continuous(name = focusVar, type = "viridis") +
  geom_line(data = nd, mapping = aes(group = get(focusVar))) + 
  theme_bw()
```




# Cross-validation

- A very flexible framework that tests how well a model can predict independent (unseen) data
- Data is typically split into two set; one for model fitting (*training data*) and one for prediction (*validation data*)
- Performance can be evaluated based on the desired metric 
- Can be used as a basis for selecting best performing model (e.g. inclusion of terms, model structure) as it can evaluate overfitting
- Especially prevalent in the contexts of prediction and evaluation of models built on small data sets




## Common metrics

Below are some common metrics that could be used for model selection. The choice of metric will depend on the goal of the model prediction. For example, 

Note that these metrics may differ from those by the model's fitting criteria to the training data set. Consistency between the two may be desirable, although one might want to emphasize other aspects of the prediction (validation) data set. 

For example, squared error metrics, like Mean Squared Error ($MSE$) and Root Mean Squared Error ($RMSE$), will more heavily penalize outliers and large errors. Mean Absolute Error $MAE$ an intuitive metric that gives average error in the same units as the data. Mean Absolute Percentage Error ($MAPE$) and  Mean Absolute Scaled Error ($MASE$) provide a scale-independent metric, useful when data varies significantly in magnitude. 

Below are some R functions that can return these metrics, with arguments `y` and `yhat` providing the observed and predicted response values, respectively, of the validation data. 


```{r metricFuns}
# Mean Squared Error (MSE)
mse <- function(y, yhat){mean((y - yhat)^2, na.rm = TRUE)}

# Root Mean Squared Error (RMSE)
rmse <- function(y, yhat){sqrt(mean((y - yhat)^2, na.rm = TRUE))}

# Mean Absolute Percentage Error (MAPE)
mape <- function(y, yhat){mean(abs(y - yhat) / y, na.rm = TRUE)}

# Median Absolute Percentage Error (MAPE)
mdape <- function(y, yhat){median(abs(y - yhat) / y, na.rm = TRUE)}

# Mean Absolute Error (MAE)
mae <- function(y, yhat){mean(abs(y - yhat), na.rm = TRUE)}

# Mean Absolute Scaled Error (MASE)
mase <- function(y, yhat, yhat_naive){mean(abs(y - yhat), na.rm = TRUE) /
    mean(abs(y - yhat_naive), na.rm = TRUE)}
```


## Repeated k-fold

Many flavors of cross validation exist, but we will focus on one of the most common and flexible ones -- *k-fold cross validation*.

In this approach, the *k* signifies the number of random groupings (i.e. *folds*) to use for the training and validation data. The groups are balanced, such that each data sample will be used once for validation. 

For example, the commonly used 5-fold cross validation divides the data into 5 groups, with each fold using 4 out of 5 of the groups for training (~80\% of data) and the remaining group for validation (~20\% of data).

The number of folds chosen depends on several considerations, such as the variance in the data, data size, and computational cost. A low k (e.g. 5-fold) had typically provides a good balance for the types of models that we have looked at. The larger amount of validation data provides a robust measure of prediction error, and the low number of folds is not computationally costly. That said, performing repeated k-fold CV (i.e. multiple permutations) reduces variability in error estimation.

### Functions

```{r cvFuncions}
# Determines k-fold partitions for a given number of samples
# n is the number of samples; k is the number of partitions
kfold <- function(n, k = NULL){
  if(is.null(k)){k <- n} # if undefined, assume leave-one-out (LOO) CV
  res <- vector(mode="list", k)
  n.remain <- seq(n)
  for(i in seq(k)){
    samp <- sample(seq(length(n.remain)), ceiling(length(n.remain)/(k-i+1)))
    res[[i]] <- n.remain[samp]
    n.remain <- n.remain[-samp]
  }
  return(res)
}

# perform repeated k-fold cross validation
# n and k are passed to kfold; seed is a integer value passed to set.seed
cvFun <- function(model = NULL, nperm = 10, k = 5, seed = 1){
  modelData <- model.frame(model)
  y <- model.response(model.frame(model))
  fmla <- formula(model)
  res <- vector("list", nperm)
  set.seed(seed)
  for(j in seq(nperm)){ # for each permutation j
    ks <- kfold(n = nrow(modelData), k = k)
    res[[j]] <- data.frame(perm = j, k = NaN, y = y, yhat = NaN)
    for(i in seq(ks)){ # for each partition k
      train <- -ks[[i]]
      valid <- ks[[i]]
      # update model with training data
      fit.k <- update(object = model, data = modelData[train,], 
        formula = as.formula(fmla, env = environment())) 
      # predict validation data
      res[[j]]$yhat[valid] <- predict(object = fit.k, 
        newdata = modelData[valid,], type = "response")
      res[[j]]$k[valid] <- i
    }
  }
  res <- do.call("rbind", res)
  return(res)
}
```


### k-fold partitioning

```{r kfoldExample}
ks <- kfold(n = nrow(dat), k = 5)
df <- as.data.frame(do.call("cbind", ks))
names(df) <- seq(ncol(df))

df <- pivot_longer(df, cols = 1:5)
df <- df[order(df$value),]
names(df) <- c("k", "t")

df = merge(dat, df, all = TRUE)

newdat <- data.frame(ssb = seq(0, max(df$ssb), len = 100))
pred <- vector("list", length(ks))
for (i in seq_along(ks)) {
  train <- -ks[[i]]
  valid <- ks[[i]]
  fit.k <- update(fit0, data = df[train,], formula = as.formula(fixed_variables, env = environment()))
  newdat.i <- newdat
  newdat.i$k <- as.character(i)
  newdat.i$rec <- predict(fit.k, newdata = newdat.i, type = "response")
  pred[[i]] <- newdat.i
}
pred <- do.call("rbind", pred)

pred$k <- as.character(pred$k)
df$k <- as.character(df$k)

p1 <- ggplot(df) + aes(x = t, y = 0, col = factor(k)) + 
  geom_point() +
  scale_color_brewer(palette = "Set1", name = "Fold") +
  labs(y = "") +
  scale_y_continuous(limits = c(0, 0), breaks = NULL) + 
  theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
    axis.ticks.y = element_blank(), axis.line.y = element_blank())

p2 <- ggplot(df) + aes(x = ssb, y = rec, col = factor(k)) + 
  geom_point() +
  geom_line(data = pred, show.legend = F) +
  lims(x = c(0,NA), y = c(0,NA)) +
  scale_color_brewer(palette = "Set1", name = "Fold")

layout = "
AA
BB
BB
"

p <- (p1 / p2) + plot_layout(design = layout, guides = "collect") &
  theme_bw()

p

```

Note that the number of folds will influence the resulting metric of fit to the validation set. The more folds, the closer one gets to the error of the data, with a leave-one-out cross validation (*LOOCV*) being the least biased.

```{r kfoldErrorEst}
n <- 20
set.seed(123)
x <- runif(n, min = 0, max = 40)
err <- rnorm(n, sd = 15)
y <- 10 + 2*x + err
df <- data.frame(x, y)
fitlm <- lm(y ~ x, df)

res <- data.frame(k = c(round(exp(seq(log(2),log(n/2), len = 10))),n), rmse = NaN, sdResid = NaN)
for(i in seq(nrow(res))){
  tmp <- cvFun(model = fitlm, nperm = 50, k = res$k[i], seed = 1)
  res$rmse[i] <- rmse(y = tmp$y, yhat = tmp$yhat) # sd of residuals
  res$sdResid[i] <- sd(tmp$y - tmp$yhat) # sd of residuals
}

p1 <- ggplot(df) + aes(x = x, y = y) + 
  geom_point() + 
  geom_abline(intercept = coef(fitlm)[1], slope = coef(fitlm)[2])

p2 <- ggplot(res) + aes(x = k, y = rmse) + 
  geom_line() + 
  geom_point() + 
  geom_hline(yintercept = sqrt(mean(fitlm$residuals^2)), linetype = 2, col = 2) +
  geom_hline(yintercept = sqrt(mean(err^2)), linetype = 2, col = 4) + 
  geom_text(x = n/2, y = sqrt(mean(fitlm$residuals^2)), 
    label = "Model error", col = 2, vjust = -0.25) + 
  geom_text(x = n/2, y = sqrt(mean(err^2)), 
    label = "True error", col = 4, vjust = -0.25)

p <- (p1 | p2) & theme_bw()
p
  
```


## Model selection

You can compare models of different structure as well. 
For example, AIC may not be comparible between models of different structure, e.g. mixed models with random effects.
Comparison should be made using same folds (e.g. use of random seed). 
A variety of metrics could be used for the final model selection.

```{r cvModelCompare, warning=FALSE}
res0 <- cvFun(model = fit0, nperm = 5, k = 5, seed = 1111)
res <- cvFun(model = fit, nperm = 5, k = 5, seed = 1111)

# mixed model with autoregressive term on time (t)
fmlatmp <- formula(fit)
fmlaMM <- as.formula(paste(c(deparse(fmlatmp), "+ ar1(t + 0 | 1)")))
fitMM <- glmmTMB(fmlaMM, data = dat, family = gaussian(link = "log"))
resMM <- cvFun(model = fitMM, nperm = 5, k = 5, seed = 1111)


cvStats0 <- with(res0, c(
  mse = mse(y = y, yhat = yhat),
  rmse = rmse(y = y, yhat = yhat),
  mape = mape(y = y, yhat = yhat),
  mdape = mdape(y = y, yhat = yhat),
  mae = mae(y = y, yhat = yhat)
))

cvStats <- with(res, c(
  mse = mse(y = y, yhat = yhat),
  rmse = rmse(y = y, yhat = yhat),
  mape = mape(y = y, yhat = yhat),
  mdape = mdape(y = y, yhat = yhat),
  mae = mae(y = y, yhat = yhat)
))

cvStatsMM <- with(resMM, c(
  mse = mse(y = y, yhat = yhat),
  rmse = rmse(y = y, yhat = yhat),
  mape = mape(y = y, yhat = yhat),
  mdape = mdape(y = y, yhat = yhat),
  mae = mae(y = y, yhat = yhat)
))

tmp <- cbind(cvStats0, cvStats, cvStatsMM)
tmp <- as.data.frame(t(apply(tmp, 1, FUN = function(x){
  paste0(format(x, scientific = TRUE, digits = 3), 
    ifelse(x == min(x, na.rm = T), "*", " "))
})))

names(tmp) <- c("res0", "res", "resMM")
tmp
```
In this case, the addition of the random intercept with an autoregressive structure based on time (t) did not improve the model for any of the evaluated metrics.





## Covariate selection with machine learning

[cost function](https://en.wikipedia.org/wiki/Loss_function)

Number of model combinations is $combinations = 2^n-1$, and here we have n = `r length(envvars)`  covariates and `r 2^(length(envvars))-1` possible combinations.

### Fitness function

```{r fitnessFun}
fitnessFun <- function(genes = NULL, data = NULL, nperm = 10, k = 5, 
  ffSeed = 1, fitnessOnly = TRUE){

  fmla <- as.formula(
    paste(c("rec ~ offset(log(ssb)) + ssb", envvars[as.logical(genes)]), collapse = " + "), 
    env = environment())
  fit <- glm(formula = fmla, data = data, family = gaussian(link = "log"))
  
  res <- cvFun(model = fit, nperm = nperm, k = k, seed = ffSeed)
  
  fitness <- -1 * mape(y = res$y, yhat = res$yhat) # to maximize, so reverse sign 
  if(!fitnessOnly){
    byPerm <- res %>% group_by(perm) %>% summarise(mape = mape(y = y, yhat = yhat))
  }
  if(fitnessOnly){
    out <- fitness
  }else{
    out <- list(fitness = fitness, byPerm = byPerm)
  }
  return(out)
}
```

```{r startPop}
popSize <- max(30, length(envvars)) # number of individuals

PARS <- seq(envvars)*0
startPop <- matrix(0, ncol = length(PARS), nrow = popSize)
diag(startPop[seq(PARS),]) <- 1
if(popSize > length(PARS)){
  hit <- seq(nrow(startPop))[-seq(PARS)]
  startPop[hit, ] <- t(apply(startPop[hit, ], 1, 
    FUN = function(x){x[sample(seq(PARS), size = 2)] <- 1; x}))
}

tmp <- startPop
tmp[] <- c("off", "on")[c(tmp)+1]
df <- as.data.frame(tmp)
names(df) <- envvars
df$ind <- seq(nrow(df))

df_long <- df %>%
  pivot_longer(cols = -ind, names_to = "variable", values_to = "value")
  

ggplot(df_long, aes(x = variable, y = ind, fill = factor(value))) +
  geom_tile(colour = 1) +
  scale_fill_manual(name = "Gene state", values = c("black", "white"), breaks = c("on", "off")) +   # Use color scale (e.g., viridis)
  scale_y_reverse(expand = c(0, 0)) + 
  scale_x_discrete(expand = c(0, 0)) +
  labs(x = "Genes (covariates)", y = "Individuals", title = "Starting population") +
  theme_bw()

```


Distribution of fitness for starting population

### Check fitness of starting pop

```{r}

tmp1 <- apply(startPop, 1, FUN = function(x){fitnessFun(x, data = dat)})
tmp2 <- apply(startPop, 1, FUN = function(x){paste(envvars[as.logical(x)], collapse = "+")})
tmp <- data.frame(ind = seq(tmp1), fitness = tmp1, covariates = tmp2)

ggplot(tmp) + aes(x = ind, y = fitness, label = covariates) + 
  geom_point() + 
  geom_text(angle = 90, hjust = -0.1, vjust = 0.5) + 
  scale_y_continuous(expand = expansion(mult = c(0.05,0.25))) + 
  theme_bw()

```

The fitness of the model with "x01" alone is already a clear standout with highest fitness (lowest mape).



### Genetic algorithm

Next, we will provide this starting population to a genetic algorithm (`GA::ga()`) of type "binary", which will turn off/on the covariates, and test the resulting fitness. 
According to the [vignette of the GA package](https://cran.r-project.org/web/packages/GA/vignettes/GA.html):

> *Genetic algorithms (GAs) are stochastic search algorithms inspired by the basic principles of biological evolution and natural selection. GAs simulate the evolution of living organisms, where the fittest individuals dominate over the weaker ones, by mimicking the biological mechanisms of evolution, such as selection, crossover and mutation.*

Likewise, these aspects can be controlled by the `ga` function with secific arguments, e.g. the probability of gene crossover (`pcrossover`), the probability of mutation (`pmutation`), etc. 

```{r message=FALSE}
# extra diagnostic data object to save all generations
postfit <- function(object, ...){
  pop <- as.data.frame(object@population)
  names(pop) <- paste0("par", seq(ncol(pop)))
  pop$fitness <- object@fitness
  pop$iter <- object@iter
  # update info
  if(!exists(".pop", envir = globalenv()))
    assign(".pop", NULL, envir = globalenv())
  .pop <- get(".pop", envir = globalenv())
  assign(".pop", rbind(.pop, pop), envir = globalenv()) 
  # output the input ga object (this is needed!)
  object 
}

mfitnessFun <- memoise(fitnessFun) # a memoised version of the fitness function
.pop <- NULL # make sure diagnostic output is empty
ga.fit <- ga(
  type = "binary", # for covariate selection
  fitness = fitnessFun, # fitness function
  nBits = length(envvars), # number of genes (covariates)
  suggestions = startPop, # starting population suggestions
  popSize = popSize, # population size
  pcrossover = 0.8, pmutation = 0.1, elitism = popSize*0.05, # default settings
  parallel = round(parallel::detectCores()/2), # parallel computing using half of available cores
  maxiter = 100, # max number of generations
  run = 40, # the max number of generations without any improvement
  monitor = FALSE, # set to TRUE to monitor fitness evolution
  data = dat, # input data
  seed = 1, # for reproducibility
  postFitness = postfit # extra diagnostics function
)
tmp <- forget(mfitnessFun) # to stop memoisation

plot(ga.fit) # plot fitness development
```

### Pareto front

```{r paretoPlot}
.pop$nVar <- rowSums(.pop[,seq(envvars)])
.pop$fracVar <- .pop$nVar / length(envvars)
names(.pop)[seq(envvars)] <- envvars

# pareto front
agg <- aggregate(fitness ~ fracVar, data = .pop, FUN = max)

# determine standard error of pareto front models
parFront <- merge(x = agg, y = .pop, all.x = T)
parFront <- unique(parFront[,c(envvars, "nVar", "fracVar", "fitness")])
parFront <- parFront[order(parFront$nVar),]
parFront

parFront$se <- NaN
for(i in seq(nrow(parFront))){
  genes.i <- parFront[i,envvars]
  tmp <- fitnessFun(genes = genes.i, data = dat, nperm = 10, k = 5, ffSeed = 1, fitnessOnly = F)
  parFront$se[i] <- sd(tmp$byPerm$mape)/sqrt(nrow(tmp$byPerm))
}

upr <- parFront$fitness + parFront$se
lwr <- parFront$fitness - parFront$se
parFront$best <- FALSE
hit <- max(which(lwr > c(-Inf, upr[-nrow(parFront)])))
parFront$best[hit] <- TRUE

ggplot(.pop) + aes(x = -fitness, y = fracVar, color = iter) + 
  geom_point(size = 3) + 
  # scale_color_viridis_c(name = "Iteration", option = "magma", direction = -1) +
  scale_color_continuous(name = "Iteration", type = "viridis", direction = -1) +
  geom_path(data = parFront, color = 4, linetype = 2) +
  geom_label(data = parFront[1,], color = 1, label = "Ricker", 
    hjust = -0.5, vjust = 0.5) +
  geom_segment(data = parFront, aes(x = -fitness-se, xend = -fitness+se, y = fracVar, yend = fracVar), 
    color = 4, linewidth = 1) +
  geom_text(data = subset(parFront, best), color = "red", label = "*", size = 10, hjust = 1.25) +
  labs(x = "MAPE", y = "Fraction of environmental covariates", title = "Pareto front") + 
  theme_bw()

```


```{r paretoFront}
parFront
```


```{r bestModel, include=FALSE}
best <- subset(parFront, best)[envvars]
best <- paste(c(fixed_variables, names(best)[best == 1]), collapse = " + ")
```
Best model is `r best`.


## Block

To address potential autocorrelation

```{r determineAutocor}
acf(dat$rec)
```


## Holdout and naive comparison

The resulting performance metrics from CV with be influenced by the number of folds, blocks etc. chosen, and does not accurately reflect the expected predictive error to new observations.

e.g. the RMSE of the validation set will be lower when using a higher number of folds (more training data, less validation)

Furthermore, in a typical training / validation CV approach, the model *sees* every data point at least once, which may introduce a bias to expected prediction performance.

Thus, a new category of data splitting, the "holdout", defines data that is kept separate of the model selection process, which can then be used to evaluate more realistic prediction performance. In the MSE context, the ability to forecast into the future is desired.

Finally, the forecast performance on 



Steps and strategies - Covariate & model selection - Cross

validation strategies - Flavours of splitting (random, block,

holdout) - Metrics for cost function (RMSE, MAE, MdAPE, …) -

Automated covariate selection strategies - Genetic Algorithm -

Identifying parsimonious model - AIC - Pareto front - Rules of

thumb (Tibshirani) - Hindcast cross validation performance -

Relating to holdout CV strategy - Comparison to naive model

(Mean Absolute Scaled Error (MASE))


Practical : predictive capability assessment – technical aspects –

also illustrating different strategies and consequences



# Caveats

- Predicting outside the observed range
  - Mechanistic-based models may be more constrained when new covariate values beyond the historical range are used for prediction
- detrending data


# Other related topics
- NSGA-II
- memoisation

[@kuhn_using_2021]



# Software Versions

- `r version$version.string`
- parallel: `r packageVersion('parallel')`
- forecast: `r packageVersion('parallel')`
- tidyr: `r packageVersion('tidyr')`
- dplyr: `r packageVersion('dplyr')`
- ggplot2: `r packageVersion('ggplot2')`
- corrplot: `r packageVersion('corrplot')`
- GA: `r packageVersion('GA')`
- knitr: `r packageVersion('knitr')`
- glmmTMB: `r packageVersion('glmmTMB')`
- doRNG: `r packageVersion('doRNG')`
- memoise: `r packageVersion('memoise')`
- **Compiled**: `r format(Sys.Date(), '%Y-%b-%d')`


# Author

**Marc Taylor**. Thünen Institute of Sea Fisheries, Marine Living Resources Unit, Herwigstraße 31, 27572 Bremerhaven, Germany. <https://www.thuenen.de/en/sf/>

# References




